{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af542eda-1c2f-4895-bb82-76b4eecf5bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import argparse\n",
    "\n",
    "\n",
    "# For information on AzureML packages: https://docs.microsoft.com/en-us/python/api/?view=azure-ml-py\n",
    "from azureml.training.tabular._diagnostics import logging_utilities\n",
    "\n",
    "\n",
    "def setup_instrumentation(automl_run_id):\n",
    "    import logging\n",
    "    import sys\n",
    "\n",
    "    from azureml.core import Run\n",
    "    from azureml.telemetry import INSTRUMENTATION_KEY, get_telemetry_log_handler\n",
    "    from azureml.telemetry._telemetry_formatter import ExceptionFormatter\n",
    "\n",
    "    logger = logging.getLogger(\"azureml.training.tabular\")\n",
    "\n",
    "    try:\n",
    "        logger.setLevel(logging.INFO)\n",
    "\n",
    "        # Add logging to STDOUT\n",
    "        stdout_handler = logging.StreamHandler(sys.stdout)\n",
    "        logger.addHandler(stdout_handler)\n",
    "\n",
    "        # Add telemetry logging with formatter to strip identifying info\n",
    "        telemetry_handler = get_telemetry_log_handler(\n",
    "            instrumentation_key=INSTRUMENTATION_KEY, component_name=\"azureml.training.tabular\"\n",
    "        )\n",
    "        telemetry_handler.setFormatter(ExceptionFormatter())\n",
    "        logger.addHandler(telemetry_handler)\n",
    "\n",
    "        # Attach run IDs to logging info for correlation if running inside AzureML\n",
    "        try:\n",
    "            run = Run.get_context()\n",
    "            return logging.LoggerAdapter(logger, extra={\n",
    "                \"properties\": {\n",
    "                    \"codegen_run_id\": run.id,\n",
    "                    \"automl_run_id\": automl_run_id\n",
    "                }\n",
    "            })\n",
    "        except Exception:\n",
    "            pass\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    return logger\n",
    "\n",
    "\n",
    "automl_run_id = 'tesla_stock_264'\n",
    "logger = setup_instrumentation(automl_run_id)\n",
    "\n",
    "\n",
    "def split_dataset(X, y, weights, split_ratio, should_stratify):\n",
    "    '''\n",
    "    Splits the dataset into a training and testing set.\n",
    "\n",
    "    Splits the dataset using the given split ratio. The default ratio given is 0.25 but can be\n",
    "    changed in the main function. If should_stratify is true the data will be split in a stratified\n",
    "    way, meaning that each new set will have the same distribution of the target value as the\n",
    "    original dataset. should_stratify is true for a classification run, false otherwise.\n",
    "    '''\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    random_state = 42\n",
    "    if should_stratify:\n",
    "        stratify = y\n",
    "    else:\n",
    "        stratify = None\n",
    "\n",
    "    if weights is not None:\n",
    "        X_train, X_test, y_train, y_test, weights_train, weights_test = train_test_split(\n",
    "            X, y, weights, stratify=stratify, test_size=split_ratio, random_state=random_state\n",
    "        )\n",
    "    else:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, stratify=stratify, test_size=split_ratio, random_state=random_state\n",
    "        )\n",
    "        weights_train, weights_test = None, None\n",
    "\n",
    "    return (X_train, y_train, weights_train), (X_test, y_test, weights_test)\n",
    "\n",
    "\n",
    "def get_training_dataset(dataset_uri):\n",
    "    \n",
    "    from azureml.core.run import Run\n",
    "    from azureml.data.abstract_dataset import AbstractDataset\n",
    "    \n",
    "    logger.info(\"Running get_training_dataset\")\n",
    "    ws = Run.get_context().experiment.workspace\n",
    "    dataset = AbstractDataset._load(dataset_uri, ws)\n",
    "    return dataset.to_pandas_dataframe()\n",
    "\n",
    "\n",
    "def prepare_data(dataframe):\n",
    "    '''\n",
    "    Prepares data for training.\n",
    "    \n",
    "    Cleans the data, splits out the feature and sample weight columns and prepares the data for use in training.\n",
    "    This function can vary depending on the type of dataset and the experiment task type: classification,\n",
    "    regression, or time-series forecasting.\n",
    "    '''\n",
    "    \n",
    "    from azureml.training.tabular.preprocessing import data_cleaning\n",
    "    \n",
    "    logger.info(\"Running prepare_data\")\n",
    "    label_column_name = 'Volume'\n",
    "    \n",
    "    # extract the features, target and sample weight arrays\n",
    "    y = dataframe[label_column_name].values\n",
    "    X = dataframe.drop([label_column_name], axis=1)\n",
    "    sample_weights = None\n",
    "    X, y, sample_weights = data_cleaning._remove_nan_rows_in_X_y(X, y, sample_weights,\n",
    "     is_timeseries=True, target_column=label_column_name)\n",
    "    \n",
    "    return X, y, sample_weights\n",
    "\n",
    "\n",
    "def generate_data_transformation_config():\n",
    "    from azureml.training.tabular.featurization._featurization_config import FeaturizationConfig\n",
    "    from azureml.training.tabular.featurization.timeseries.category_binarizer import CategoryBinarizer\n",
    "    from azureml.training.tabular.featurization.timeseries.lag_lead_operator import LagLeadOperator\n",
    "    from azureml.training.tabular.featurization.timeseries.max_horizon_featurizer import MaxHorizonFeaturizer\n",
    "    from azureml.training.tabular.featurization.timeseries.missingdummies_transformer import MissingDummiesTransformer\n",
    "    from azureml.training.tabular.featurization.timeseries.numericalize_transformer import NumericalizeTransformer\n",
    "    from azureml.training.tabular.featurization.timeseries.restore_dtypes_transformer import RestoreDtypesTransformer\n",
    "    from azureml.training.tabular.featurization.timeseries.rolling_window import RollingWindow\n",
    "    from azureml.training.tabular.featurization.timeseries.short_grain_dropper import ShortGrainDropper\n",
    "    from azureml.training.tabular.featurization.timeseries.time_index_featurizer import TimeIndexFeaturizer\n",
    "    from azureml.training.tabular.featurization.timeseries.time_series_imputer import TimeSeriesImputer\n",
    "    from azureml.training.tabular.featurization.timeseries.timeseries_transformer import TimeSeriesPipelineType\n",
    "    from azureml.training.tabular.featurization.timeseries.timeseries_transformer import TimeSeriesTransformer\n",
    "    from azureml.training.tabular.featurization.timeseries.unique_target_grain_dropper import UniqueTargetGrainDropper\n",
    "    from collections import OrderedDict\n",
    "    from numpy import dtype\n",
    "    from numpy import nan\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    \n",
    "    transformer_list = []\n",
    "    transformer1 = UniqueTargetGrainDropper(\n",
    "        cv_step_size=7,\n",
    "        max_horizon=7,\n",
    "        n_cross_validations=5,\n",
    "        target_lags=[1],\n",
    "        target_rolling_window_size=2\n",
    "    )\n",
    "    transformer_list.append(('unique_target_grain_dropper', transformer1))\n",
    "    \n",
    "    transformer2 = MissingDummiesTransformer(\n",
    "        numerical_columns=['Column1', 'Open', 'High', 'Low', 'Close']\n",
    "    )\n",
    "    transformer_list.append(('make_numeric_na_dummies', transformer2))\n",
    "    \n",
    "    transformer3 = TimeSeriesImputer(\n",
    "        end=None,\n",
    "        freq='D',\n",
    "        impute_by_horizon=False,\n",
    "        input_column=['Column1', 'Open', 'High', 'Low', 'Close'],\n",
    "        limit=None,\n",
    "        limit_direction='forward',\n",
    "        method=OrderedDict([('ffill', [])]),\n",
    "        option='fillna',\n",
    "        order=None,\n",
    "        origin=None,\n",
    "        value={'Column1': 1766.5, 'Open': 17.417333602905273, 'High': 17.645333290100098, 'Low': 17.064000129699707, 'Close': 17.400333404541016}\n",
    "    )\n",
    "    transformer_list.append(('impute_na_numeric_datetime', transformer3))\n",
    "    \n",
    "    transformer4 = ShortGrainDropper(\n",
    "        cv_step_size=7,\n",
    "        max_horizon=7,\n",
    "        n_cross_validations=5,\n",
    "        target_lags=[1],\n",
    "        target_rolling_window_size=2\n",
    "    )\n",
    "    transformer_list.append(('grain_dropper', transformer4))\n",
    "    \n",
    "    transformer5 = RestoreDtypesTransformer(\n",
    "        dtypes={'_automl_target_col': dtype('int64'), 'Column1': dtype('int64'), 'Close': dtype('float64'), 'Open': dtype('float64'), 'Low': dtype('float64'), 'High': dtype('float64')},\n",
    "        target_column='_automl_target_col'\n",
    "    )\n",
    "    transformer_list.append(('restore_dtypes_transform', transformer5))\n",
    "    \n",
    "    transformer6 = MaxHorizonFeaturizer(\n",
    "        freq='D',\n",
    "        horizon_colname='horizon_origin',\n",
    "        max_horizon=7,\n",
    "        origin_time_colname='origin'\n",
    "    )\n",
    "    transformer_list.append(('max_horizon_featurizer', transformer6))\n",
    "    \n",
    "    transformer7 = LagLeadOperator(\n",
    "        backfill_cache=False,\n",
    "        dropna=False,\n",
    "        freq='D',\n",
    "        lags={'_automl_target_col': [1]},\n",
    "        max_horizon=7,\n",
    "        origin_time_column_name='origin',\n",
    "        overwrite_columns=True\n",
    "    )\n",
    "    transformer_list.append(('lag_lead_operator', transformer7))\n",
    "    \n",
    "    transformer8 = RollingWindow(\n",
    "        backfill_cache=False,\n",
    "        check_max_horizon=False,\n",
    "        dropna=False,\n",
    "        freq='D',\n",
    "        max_horizon=7,\n",
    "        origin_time_column_name='origin',\n",
    "        transform_dictionary={'min': '_automl_target_col', 'max': '_automl_target_col', 'mean': '_automl_target_col'},\n",
    "        transform_options={},\n",
    "        window_options={'center': False, 'closed': None},\n",
    "        window_size=2\n",
    "    )\n",
    "    transformer_list.append(('rolling_window_operator', transformer8))\n",
    "    \n",
    "    transformer9 = NumericalizeTransformer(\n",
    "        categories_by_col={},\n",
    "        exclude_columns=set(),\n",
    "        include_columns=set()\n",
    "    )\n",
    "    transformer_list.append(('make_categoricals_numeric', transformer9))\n",
    "    \n",
    "    transformer10 = TimeIndexFeaturizer(\n",
    "        correlation_cutoff=0.99,\n",
    "        country_or_region=None,\n",
    "        datetime_columns=None,\n",
    "        force_feature_list=None,\n",
    "        freq='D',\n",
    "        holiday_end_time=None,\n",
    "        holiday_start_time=None,\n",
    "        overwrite_columns=True,\n",
    "        prune_features=True\n",
    "    )\n",
    "    transformer_list.append(('make_time_index_featuers', transformer10))\n",
    "    \n",
    "    transformer11 = CategoryBinarizer(\n",
    "        columns=[],\n",
    "        drop_first=False,\n",
    "        dummy_na=False,\n",
    "        encode_all_categoricals=False,\n",
    "        prefix=None,\n",
    "        prefix_sep='_'\n",
    "    )\n",
    "    transformer_list.append(('make_categoricals_onehot', transformer11))\n",
    "    \n",
    "    pipeline = Pipeline(steps=transformer_list)\n",
    "    tst = TimeSeriesTransformer(\n",
    "        country_or_region=None,\n",
    "        drop_column_names=[],\n",
    "        featurization_config=FeaturizationConfig(\n",
    "            blocked_transformers=None,\n",
    "            column_purposes=None,\n",
    "            dataset_language=None,\n",
    "            prediction_transform_type=None,\n",
    "            transformer_params=None\n",
    "        ),\n",
    "        force_time_index_features=None,\n",
    "        freq='D',\n",
    "        grain_column_names=None,\n",
    "        group=None,\n",
    "        lookback_features_removed=False,\n",
    "        max_horizon=7,\n",
    "        origin_time_colname='origin',\n",
    "        pipeline=pipeline,\n",
    "        pipeline_type=TimeSeriesPipelineType.FULL,\n",
    "        seasonality=7,\n",
    "        time_column_name='Date',\n",
    "        time_index_non_holiday_features=['_automl_year', '_automl_half', '_automl_quarter', '_automl_month', '_automl_day', '_automl_wday', '_automl_qday', '_automl_week'],\n",
    "        use_stl=None\n",
    "    )\n",
    "    \n",
    "    return tst\n",
    "    \n",
    "    \n",
    "def generate_preprocessor_config_0():\n",
    "    '''\n",
    "    Specifies a preprocessing step to be done after featurization in the final scikit-learn pipeline.\n",
    "    \n",
    "    Normally, this preprocessing step only consists of data standardization/normalization that is\n",
    "    accomplished with sklearn.preprocessing. Automated ML only specifies a preprocessing step for\n",
    "    non-ensemble classification and regression models.\n",
    "    '''\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    \n",
    "    preproc = StandardScaler(\n",
    "        copy=True,\n",
    "        with_mean=False,\n",
    "        with_std=False\n",
    "    )\n",
    "    \n",
    "    return preproc\n",
    "    \n",
    "    \n",
    "def generate_algorithm_config_0():\n",
    "    from xgboost.sklearn import XGBRegressor\n",
    "    \n",
    "    algorithm = XGBRegressor(\n",
    "        base_score=0.5,\n",
    "        booster='gbtree',\n",
    "        colsample_bylevel=1,\n",
    "        colsample_bynode=1,\n",
    "        colsample_bytree=1,\n",
    "        enable_categorical=False,\n",
    "        gamma=0,\n",
    "        gpu_id=-1,\n",
    "        importance_type=None,\n",
    "        interaction_constraints='',\n",
    "        learning_rate=0.300000012,\n",
    "        max_delta_step=0,\n",
    "        max_depth=6,\n",
    "        min_child_weight=1,\n",
    "        missing=numpy.nan,\n",
    "        monotone_constraints='()',\n",
    "        n_estimators=100,\n",
    "        n_jobs=0,\n",
    "        num_parallel_tree=1,\n",
    "        objective='reg:squarederror',\n",
    "        predictor='auto',\n",
    "        random_state=0,\n",
    "        reg_alpha=0,\n",
    "        reg_lambda=1,\n",
    "        scale_pos_weight=1,\n",
    "        subsample=1,\n",
    "        tree_method='auto',\n",
    "        validate_parameters=1,\n",
    "        verbose=-10,\n",
    "        verbosity=0\n",
    "    )\n",
    "    \n",
    "    return algorithm\n",
    "    \n",
    "    \n",
    "def generate_algorithm_config_1():\n",
    "    from azureml.training.tabular.models.forecasting_models import Naive\n",
    "    from numpy import array\n",
    "    \n",
    "    algorithm = Naive(\n",
    "        timeseries_param_dict={'time_column_name': 'Date', 'grain_column_names': None, 'target_column_name': 'Volume', 'drop_column_names': [], 'overwrite_columns': True, 'dropna': False, 'transform_dictionary': {'min': '_automl_target_col', 'max': '_automl_target_col', 'mean': '_automl_target_col'}, 'max_horizon': 7, 'origin_time_colname': 'origin', 'country_or_region': None, 'n_cross_validations': 5, 'short_series_handling': True, 'max_cores_per_iteration': -1, 'feature_lags': None, 'target_aggregation_function': None, 'cv_step_size': 7, 'window_size': 'auto', 'lags': {'_automl_target_col': ['auto']}, 'iteration_timeout_minutes': 30, 'seasonality': 7, 'use_stl': None, 'freq': 'D', 'short_series_handling_configuration': 'auto', 'target_lags': [1], 'target_rolling_window_size': 2, 'arimax_raw_columns': ['Close', 'Open', 'Date', 'Low', 'High', 'Column1'], 'lagging_columns': ['_automl_target_col_occurrence_lag1D'], 'rolling_window_columns': ['_automl_target_col_min_window2D', '_automl_target_col_max_window2D', '_automl_target_col_mean_window2D']}\n",
    "    )\n",
    "    \n",
    "    return algorithm\n",
    "    \n",
    "    \n",
    "def generate_algorithm_config_2():\n",
    "    from azureml.training.tabular.models._timeseries._arimax import Arimax\n",
    "    from numpy import array\n",
    "    \n",
    "    algorithm = Arimax(\n",
    "        timeseries_param_dict={'time_column_name': 'Date', 'grain_column_names': None, 'target_column_name': 'Volume', 'drop_column_names': [], 'overwrite_columns': True, 'dropna': False, 'transform_dictionary': {'min': '_automl_target_col', 'max': '_automl_target_col', 'mean': '_automl_target_col'}, 'max_horizon': 7, 'origin_time_colname': 'origin', 'country_or_region': None, 'n_cross_validations': 5, 'short_series_handling': True, 'max_cores_per_iteration': -1, 'feature_lags': None, 'target_aggregation_function': None, 'cv_step_size': 7, 'window_size': 'auto', 'lags': {'_automl_target_col': ['auto']}, 'iteration_timeout_minutes': 30, 'seasonality': 7, 'use_stl': None, 'freq': 'D', 'short_series_handling_configuration': 'auto', 'target_lags': [1], 'target_rolling_window_size': 2, 'arimax_raw_columns': ['Close', 'Open', 'Date', 'Low', 'High', 'Column1'], 'lagging_columns': ['_automl_target_col_occurrence_lag1D'], 'rolling_window_columns': ['_automl_target_col_min_window2D', '_automl_target_col_max_window2D', '_automl_target_col_mean_window2D']}\n",
    "    )\n",
    "    \n",
    "    return algorithm\n",
    "    \n",
    "    \n",
    "def generate_algorithm_config():\n",
    "    '''\n",
    "    Specifies the actual algorithm and hyperparameters for training the model.\n",
    "    \n",
    "    It is the last stage of the final scikit-learn pipeline. For ensemble models, generate_preprocessor_config_N()\n",
    "    (if needed) and generate_algorithm_config_N() are defined for each learner in the ensemble model,\n",
    "    where N represents the placement of each learner in the ensemble model's list. For stack ensemble\n",
    "    models, the meta learner generate_algorithm_config_meta() is defined.\n",
    "    '''\n",
    "    from azureml.training.tabular.models.voting_ensemble import PreFittedSoftVotingRegressor\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    \n",
    "    pipeline_0 = Pipeline(steps=[('preproc', generate_preprocessor_config_0()), ('model', generate_algorithm_config_0())])\n",
    "    pipeline_1 = Pipeline(steps=[('model', generate_algorithm_config_1())])\n",
    "    pipeline_2 = Pipeline(steps=[('model', generate_algorithm_config_2())])\n",
    "    algorithm = PreFittedSoftVotingRegressor(\n",
    "        estimators=[\n",
    "            ('model_0', pipeline_0),\n",
    "            ('model_1', pipeline_1),\n",
    "            ('model_2', pipeline_2),\n",
    "        ],\n",
    "        weights=[0.5555555555555556, 0.3333333333333333, 0.1111111111111111]\n",
    "    )\n",
    "    \n",
    "    return algorithm\n",
    "    \n",
    "    \n",
    "def build_model_pipeline():\n",
    "    '''\n",
    "    Defines the scikit-learn pipeline steps.\n",
    "    \n",
    "    For time-series forecasting models, the scikit-learn pipeline is wrapped in a ForecastingPipelineWrapper,\n",
    "    which has some additional logic needed to properly handle time-series data depending on the applied algorithm.\n",
    "    '''\n",
    "    from azureml.training.tabular.models.forecasting_pipeline_wrapper import ForecastingPipelineWrapper\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    \n",
    "    logger.info(\"Running build_model_pipeline\")\n",
    "    pipeline = Pipeline(\n",
    "        steps=[\n",
    "            ('tst', generate_data_transformation_config()),\n",
    "            ('model', generate_algorithm_config())\n",
    "        ]\n",
    "    )\n",
    "    forecast_pipeline_wrapper = ForecastingPipelineWrapper(pipeline, stddev=[20189904.154787198, 8791864.786339315, 14267829.97480106, 33668755.30915074, 14020858.23580013])\n",
    "    \n",
    "    return forecast_pipeline_wrapper\n",
    "\n",
    "\n",
    "def train_model(X, y, sample_weights=None, transformer=None):\n",
    "    '''\n",
    "    Calls the fit() method to train the model.\n",
    "    \n",
    "    The return value is the model fitted/trained on the input data.\n",
    "    '''\n",
    "    \n",
    "    logger.info(\"Running train_model\")\n",
    "    model_pipeline = build_model_pipeline()\n",
    "    \n",
    "    model = model_pipeline.fit(X, y)\n",
    "    return model\n",
    "\n",
    "\n",
    "def calculate_metrics(model, X, y, sample_weights, X_test, y_test, cv_splits=None):\n",
    "    '''\n",
    "    Calculates the metrics that can be used to evaluate the model's performance.\n",
    "    \n",
    "    Metrics calculated vary depending on the experiment type. Classification, regression and time-series\n",
    "    forecasting jobs each have their own set of metrics that are calculated.'''\n",
    "    \n",
    "    from azureml.training.tabular.preprocessing._dataset_binning import get_dataset_bins\n",
    "    from azureml.training.tabular.score.scoring import score_forecasting\n",
    "    from azureml.training.tabular.score.scoring import score_regression\n",
    "    \n",
    "    y_pred, _ = model.forecast(X_test)\n",
    "    y_min = np.min(y)\n",
    "    y_max = np.max(y)\n",
    "    y_std = np.std(y)\n",
    "    \n",
    "    bin_info = get_dataset_bins(cv_splits, X, None, y)\n",
    "    regression_metrics_names, forecasting_metrics_names = get_metrics_names()\n",
    "    metrics = score_regression(\n",
    "        y_test, y_pred, regression_metrics_names, y_max, y_min, y_std, sample_weights, bin_info)\n",
    "    \n",
    "    try:\n",
    "        horizons = X_test['horizon_origin'].values\n",
    "    except Exception:\n",
    "        # If no horizon is present we are doing a basic forecast.\n",
    "        # The model's error estimation will be based on the overall\n",
    "        # stddev of the errors, multiplied by a factor of the horizon.\n",
    "        horizons = np.repeat(None, y_pred.shape[0])\n",
    "    \n",
    "    featurization_step = generate_data_transformation_config()\n",
    "    grain_column_names = featurization_step.grain_column_names\n",
    "    time_column_name = featurization_step.time_column_name\n",
    "    \n",
    "    forecasting_metrics = score_forecasting(\n",
    "        y_test, y_pred, forecasting_metrics_names, horizons, y_max, y_min, y_std, sample_weights, bin_info,\n",
    "        X_test, X, y, grain_column_names, time_column_name)\n",
    "    metrics.update(forecasting_metrics)\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def get_metrics_names():\n",
    "    \n",
    "    regression_metrics_names = [\n",
    "        'spearman_correlation',\n",
    "        'predicted_true',\n",
    "        'root_mean_squared_log_error',\n",
    "        'median_absolute_error',\n",
    "        'mean_absolute_percentage_error',\n",
    "        'r2_score',\n",
    "        'root_mean_squared_error',\n",
    "        'residuals',\n",
    "        'mean_absolute_error',\n",
    "        'explained_variance',\n",
    "    ]\n",
    "    forecasting_metrics_names = [\n",
    "        'forecast_residuals',\n",
    "        'forecast_adjustment_residuals',\n",
    "        'forecast_mean_absolute_percentage_error',\n",
    "        'forecast_table',\n",
    "    ]\n",
    "    return regression_metrics_names, forecasting_metrics_names\n",
    "\n",
    "\n",
    "def get_metrics_log_methods():\n",
    "    \n",
    "    metrics_log_methods = {\n",
    "        'spearman_correlation': 'log',\n",
    "        'predicted_true': 'log_predictions',\n",
    "        'root_mean_squared_log_error': 'log',\n",
    "        'median_absolute_error': 'log',\n",
    "        'mean_absolute_error': 'log',\n",
    "        'mean_absolute_percentage_error': 'log',\n",
    "        'forecast_table': 'Skip',\n",
    "        'r2_score': 'log',\n",
    "        'root_mean_squared_error': 'log',\n",
    "        'forecast_residuals': 'Skip',\n",
    "        'residuals': 'log_residuals',\n",
    "        'explained_variance': 'log',\n",
    "        'forecast_adjustment_residuals': 'Skip',\n",
    "        'forecast_mean_absolute_percentage_error': 'Skip',\n",
    "    }\n",
    "    return metrics_log_methods\n",
    "\n",
    "\n",
    "def main(training_dataset_uri=None):\n",
    "    '''\n",
    "    Runs all functions defined above.\n",
    "    '''\n",
    "    \n",
    "    from azureml.automl.core.inference import inference\n",
    "    from azureml.core.run import Run\n",
    "    from azureml.training.tabular.score._cv_splits import _CVSplits\n",
    "    from azureml.training.tabular.score.scoring import aggregate_scores\n",
    "    \n",
    "    import mlflow\n",
    "    \n",
    "    # The following code is for when running this code as part of an AzureML script run.\n",
    "    run = Run.get_context()\n",
    "    \n",
    "    df = get_training_dataset(training_dataset_uri)\n",
    "    X, y, sample_weights = prepare_data(df)\n",
    "    tst = generate_data_transformation_config()\n",
    "    tst.fit(X, y)\n",
    "    ts_param_dict = tst.parameters\n",
    "    short_series_dropper = next((step for key, step in tst.pipeline.steps if key == 'grain_dropper'), None)\n",
    "    if short_series_dropper is not None and short_series_dropper.has_short_grains_in_train and grains is not None and len(grains) > 0:\n",
    "        # Preprocess X so that it will not contain the short grains.\n",
    "        dfs = []\n",
    "        X['_automl_target_col'] = y\n",
    "        for grain, df in X.groupby(grains):\n",
    "            if grain in short_series_processor.grains_to_keep:\n",
    "                dfs.append(df)\n",
    "        X = pd.concat(dfs)\n",
    "        y = X.pop('_automl_target_col').values\n",
    "        del dfs\n",
    "    cv_splits = _CVSplits(X, y, frac_valid=None, CV=5, n_step=7, is_time_series=True, task='regression', timeseries_param_dict=ts_param_dict)\n",
    "    scores = []\n",
    "    for X_train, y_train, sample_weights_train, X_valid, y_valid, sample_weights_valid in cv_splits.apply_CV_splits(X, y, sample_weights):\n",
    "        partially_fitted_model = train_model(X_train, y_train, transformer=tst)\n",
    "        metrics = calculate_metrics(partially_fitted_model, X, y, sample_weights, X_test=X_valid, y_test=y_valid, cv_splits=cv_splits)\n",
    "        scores.append(metrics)\n",
    "        print(metrics)\n",
    "    model = train_model(X_train, y_train, transformer=tst)\n",
    "    \n",
    "    metrics = aggregate_scores(scores)\n",
    "    metrics_log_methods = get_metrics_log_methods()\n",
    "    print(metrics)\n",
    "    for metric in metrics:\n",
    "        if metrics_log_methods[metric] == 'None':\n",
    "            logger.warning(\"Unsupported non-scalar metric {}. Will not log.\".format(metric))\n",
    "        elif metrics_log_methods[metric] == 'Skip':\n",
    "            pass # Forecasting non-scalar metrics and unsupported classification metrics are not logged\n",
    "        else:\n",
    "            getattr(run, metrics_log_methods[metric])(metric, metrics[metric])\n",
    "    cd = inference.get_conda_deps_as_dict(True)\n",
    "    \n",
    "    # Saving ML model to outputs/.\n",
    "    signature = mlflow.models.signature.infer_signature(X, y)\n",
    "    mlflow.sklearn.log_model(\n",
    "        sk_model=model,\n",
    "        artifact_path='outputs/',\n",
    "        conda_env=cd,\n",
    "        signature=signature,\n",
    "        serialization_format=mlflow.sklearn.SERIALIZATION_FORMAT_PICKLE)\n",
    "    \n",
    "    run.upload_folder('outputs/', 'outputs/')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--training_dataset_uri', type=str, default='azureml://locations/centralindia/workspaces/de82e3f6-f6b5-4410-9f9c-2b198444d41c/data/telsa_stocks/versions/1',     help='Default training dataset uri is populated from the parent run')\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    try:\n",
    "        main(args.training_dataset_uri)\n",
    "    except Exception as e:\n",
    "        logging_utilities.log_traceback(e, logger)\n",
    "        raise"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
